# -*- coding: utf-8 -*-
"""cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1raKth2ow2uHbRt_hBHxRo6sIoPh6pkrQ
"""

# Importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# import plotly.express as px
# import statistics as st
import pickle

from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix


from sklearn.preprocessing import LabelEncoder

from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPool2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Dense

df = pd.read_csv('Data/hand_landmarks_data.csv')
df.head()

df.info()

df.columns[df.isnull().sum()>0]

# Data Preprocessing

import numpy as np
from sklearn.preprocessing import LabelEncoder

def normalize_landmarks(X):
    """
    Normalize landmark coordinates relative to wrist (landmark 0) & scale.

    Parameters:
        X: np.ndarray of shape (n_samples, 63)

    Returns:
        np.ndarray of shape (n_samples, 21, 3) normalized
    """
    X = X.reshape(-1, 21, 3)  # n_samples x 21 x 3
    origins = X[:, 0:1, :]    # n_samples x 1 x 3
    X -= origins              # translate wrist to (0,0,0)

    max_vals = np.max(np.abs(X), axis=(1,2), keepdims=True)  # n_samples x 1 x 1
    max_vals[max_vals == 0] = 1                              # avoid div by 0
    X /= max_vals

    return X


def preprocess_with_labels(data):
    """
    Preprocess dataset: normalize landmarks & encode labels.

    Parameters:
        data: np.ndarray of shape (n_samples, 64)

    Returns:
        X: np.ndarray of shape (n_samples, 21, 3) normalized
        Y: np.ndarray of shape (n_samples,) encoded labels
        label_encoder: fitted LabelEncoder
    """
    landmarks = data.iloc[:, :-1].to_numpy()
    labels = data.iloc[:, -1].to_numpy()


    # normalize X
    X = normalize_landmarks(landmarks)

    # encode Y
    label_encoder = LabelEncoder()
    Y = label_encoder.fit_transform(labels)

    return X, Y, label_encoder

X, Y, label_encoder = preprocess_with_labels(df)

with open('../exports/cnn_encoder.pkl', "wb") as file:
    pickle.dump(label_encoder, file)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, shuffle = True, stratify = Y, random_state = 42)

# Reshape data to match expected input shape of the CNN (28x28x1)
# Assuming your original image data is 28x28:
X_train = X_train.reshape((X_train.shape[0], 9, 7, 1))  # Reshape X_train
X_test = X_test.reshape((X_test.shape[0], 9, 7, 1))  # Reshape X_test

print(X_train.shape)
print(X_test.shape)

model=Sequential()
model.add(Conv2D(32, (3, 3), activation = 'relu', input_shape = (9, 7, 1)))

model.add(MaxPool2D(2, 2))
model.add(Flatten())

model.add(Dense(100, activation = 'relu'))
model.add(Dense(63, activation = 'softmax'))
model.add(Flatten())

model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
model.fit(X_train, Y_train, epochs = 80)

model.evaluate(X_test, Y_test)

model_filename = "../exports/cnn_model.h5"

model.save(model_filename)

print(f"Model saved to {model_filename}")
